{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTgLPQSjqhSb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDbbd4Wfr9ud",
        "outputId": "8d9e762c-a4b0-4179-a7cb-5b63e43f821d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/LLM/saved_models/fine_tuned_model.zip -d /content/drive/MyDrive/LLM/saved_models"
      ],
      "metadata": {
        "id": "U8_G5K5xsDt3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/LLM/saved_models/lora_tuned_model.zip -d /content/drive/MyDrive/LLM/saved_models"
      ],
      "metadata": {
        "id": "fEl1QVkXr6hW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Model and prompt setup\n",
        "model_name = \"distilgpt2\"\n",
        "prompt = \"Tell me a joke about programming:\"\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/LLM/saved_models/content/fine_tuned_model\")\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/LLM/saved_models/content/fine_tuned_model\")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text with randomness\n",
        "outputs = fine_tuned_model.generate(\n",
        "    inputs.input_ids,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "print(\"Fine-Tuned Model Joke:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S5qJZwmuMKR",
        "outputId": "69a481ac-65e8-433f-c68e-62243faf4657"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned Model Joke:\n",
            " Tell me a joke about programming: You can't get arrays.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qM34LKxeuxsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Model and prompt setup\n",
        "model_name = \"distilgpt2\"\n",
        "prompt = \"Tell me a joke about programming:\"\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/LLM/saved_models/content/lora_tuned_model\")\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/LLM/saved_models/content/lora_tuned_model\")\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate text with randomness\n",
        "outputs = fine_tuned_model.generate(\n",
        "    inputs.input_ids,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "# Decode and print the output\n",
        "print(\"Fine-Tuned Model Joke:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYig6HpNTj52",
        "outputId": "1b4e0fd6-39d5-40b7-e0ed-bfac399ad0a0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-Tuned Model Joke:\n",
            " Tell me a joke about programming: You don't get the array.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHyQYqthTj9Y"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}